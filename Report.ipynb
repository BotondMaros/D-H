{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docler Holding take home assignment\n",
    "Classification task - 3rd June 2020\n",
    "\n",
    "This report concludes my results and findings about the problems in take home assignment. The problems are the following: \n",
    "- use the cifar-10 dataset and a selected CNN model to classify the data (only use 1000 images for traning)\n",
    "- modify the network to produce 2 outputs\n",
    "- optimize the neural network and evaluate metrics\n",
    "\n",
    "\n",
    "This notebook only presents the result. The code base \" modified_mobilenet_1k_baseline.py \" can found at project's the directory. The libraries I used are: `Pytorch`, `numpy`, `matplotlib`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Understanding the data\n",
    "The cifar-10 dataset consists of 60000 images of 10 classes. These classes are: plane, car, bird, cat,\n",
    "deer, dog, frog, horse, ship, truck. The dataset is balanced, each class consists of 6000 images. The images were downloaded from: https://www.cs.toronto.edu/~kriz/cifar.html. The data can be found under the directory \"/data\". This contains 6 pickled data files: data_batch_1-5, and test_batch. Originally the data_batches were used for traning, but we will only use a smaller subset of the data to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modified_mobilenet_1k_baseline as base #import scripts I wrote\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's unpickle the data, extract images and labels. The we have to reshape the images as they were stored as row vectors of size 1x1024. We will reshape them to size 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "#unpickle the data , extract images and labels\n",
    "#get data to shape of 32x32\n",
    "PATH = \"data\"\n",
    "data, labels = base.load_all(PATH)\n",
    "data = base.get_shape_32x32(data)\n",
    "print(data.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataset that consisting of 60000 images, 3 color channels and 32x32 size. We can see at the following cell that we have indeed a balanced dataset, each class consists of 6000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 6000, 9: 6000, 4: 6000, 1: 6000, 2: 6000, 7: 6000, 8: 6000, 3: 6000, 5: 6000, 0: 6000})\n",
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Counter(labels))\n",
    "\n",
    "#showing one data point\n",
    "example = data[123]/255\n",
    "example = torch.Tensor(example)\n",
    "plt.imshow(example.permute(1, 2, 0))\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(labels[123]) #returns 2, classes[2] == bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare the data\n",
    "\n",
    "The task was to select 1000 images from this dataset. For this, I selected 100 images from each class. The reason I did this is I assumed that the data is i.i.d. and I can randomly sample from it, and I wanted to train the network on a balanced dataset. \n",
    "I calculated an index set which contains the indeces for traning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcualte index set: will use to split dataset\n",
    "#we will randomly sample 100 images from each class to have a balanced dataset\n",
    "indeces = base.get_100_from_each(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method used: create an empty list where we will store the labels.\n",
    "From each class select 100 occurences with np.random.choice(all_class_id,size=100,replace=False)\n",
    "\n",
    "#### Preprocessing\n",
    "Next, I divided each data point to have values between 0-1, than calcualted the mean and standard deviation for the dataset, to normalize each data point. I also splitted the dataset into train-test data and labels. The following function `base.normalize_split()` does these steps. Note that at this point I did not use any other data transformation than normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({9: 100, 5: 100, 6: 100, 4: 100, 1: 100, 2: 100, 3: 100, 8: 100, 0: 100, 7: 100})\n"
     ]
    }
   ],
   "source": [
    "#normalize each data point, split data to two PyTorch datasets \n",
    "train_data, train_labels, test_data, test_labels = base.normalize_split(indeces, data, labels)\n",
    "print(Counter(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I created a dataset and `dataLoader` PyTorch objects. These will store the data tensors and the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create PyTorch datasets with batch size 32\n",
    "train_dataloader, test_dataloader = base.create_dataset(train_data, test_data, train_labels, test_labels, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Selecting a model and modifying the model\n",
    "The model I selected to train on this dataset is the MobileNetV2. The reason I choose this model is that it achieves similar results than deeper convolutional models, but it has much less parameters to learn. This network is ideal for low resource devices (such as mobiles). This network only has ~4.2M parameters while, for example, ResNet-101 has ~58M.\n",
    "\n",
    "MobileV2Net uses an inverted residual structure, and it has depthwise and pointwise convolutional layers. (https://arxiv.org/abs/1801.04381)\n",
    "\n",
    "I also had to modify the network to have a second output that always produces ones.\n",
    "To do the following, I rewrote the forward function of the code to be:\n",
    "`def forward(self, x):\n",
    "     x = self.features(x)\n",
    "     x = x.mean([2, 3])\n",
    "     x1 = self.classifier(x)\n",
    "     x2 = torch.ones((x.shape))\n",
    "     return x1, x2`\n",
    "\n",
    "This will return two outputs, x1 being the original output of the network and x2 being a dummy tensor of the same shape as x1. This dummy only contains 1-s. \n",
    "\n",
    "To my best understanding, I did not have to train the model on both of these outputs, only on the original output. If I have to train on both the outputs, I would rewrite the following in the training:\n",
    "\n",
    "` for i, data in enumerate(train_dataloader, 0):\n",
    "    inputs, labels = data\n",
    "    optimizer.zero_grad()\n",
    "    outputs, outputs2 = net(inputs)\n",
    "    loss1 = criterion(outputs, labels)\n",
    "    loss2 = criterion(outputs2, labels) \n",
    "    loss= loss1 + loss2\n",
    "    loss.backward()\n",
    "    optimizer.step()`\n",
    "\n",
    "Next let's initialise the model with Cross-entropy loss and RMSprop optimizer.\n",
    "Cross-entropy loss is often used in multi label classificatin and it increases as the predicted probability diverges from the actual label. RMSprop is an adaptive optimizer that uses a moving average of squared gradients to 'normalise' the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = base.mobilenet_v2(pretrained=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=0.0005, momentum=0.9, weight_decay=4e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained this model in Colab \"modified_mobilenet_train.ipynb\" and saved the model parameters. The Colab notebook can be also found in the project's folder. Let's load in the model parameters and make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = 'modified_model_1k_2'\n",
    "checkpoint = torch.load(saved_model,map_location=torch.device('cpu'))\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "criterion = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "I used weight decay (L2 Regularization) in the optimizer and batch normalization in the network to optimize the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate the model\n",
    "I will plot a graph that was made during traning to show the loss. This is to avoid overfitting.\n",
    "\n",
    "I will also show the accuracy of the prediction for the whole dataset and for each classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses during traning (trained for 10 epochs):\n",
    "<img src=\"losses.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 59000 test images: 31 %\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "\n",
    "        outputs,outputs2 = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 59000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achieved accuracy on the test set is 31% which is really low, but the same model with the same parameters achieved 68%, using 50k images to train and 10k images to test. I conclude that this is not a horrible accuracy given that we only use 1k images to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4897, -5.4772,  3.1550,  3.6973,  2.1079,  3.7830,  1.3772,  0.6955,\n",
      "        -2.1807, -3.3048], grad_fn=<SelectBackward>)\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "#demonstrate that the model produces the actual output and the dummy output:\n",
    "for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs, outputs2 = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(outputs[0])\n",
    "        print(outputs2[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how many of the missclassification error concentrates in some classes. For this we will show the accuracy predicted for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "\n",
    "        outputs,outputs2 = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 56 %\n",
      "Accuracy of   car : 43 %\n",
      "Accuracy of  bird : 28 %\n",
      "Accuracy of   cat : 10 %\n",
      "Accuracy of  deer :  6 %\n",
      "Accuracy of   dog : 37 %\n",
      "Accuracy of  frog : 44 %\n",
      "Accuracy of horse : 39 %\n",
      "Accuracy of  ship : 21 %\n",
      "Accuracy of truck : 32 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for example labels truck and cat has low accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Next steps\n",
    "If I had more time, I would do the followings:\n",
    "1. Data augmentation - usually improves accuracy and helps avoiding overfitting. This would include randomCrop and flipping the images.\n",
    "2. Selecting a different subset of images. We can see that some classes perform well like plane or frog, while other classes such as cat or deer have really low accuracy. We can try to manually increase the traning examples for the low accuracy classes and decrease for higher accuracy classes. We can also take a look at the confusion matrix to see the correlation between the missclassified images.\n",
    "3. Transfer learning - using knowlegde from a different dataset\n",
    "4. Hyper-parameter tuning. This includes: epochs, batch size, optimizer parameters.\n",
    "\n",
    "5. To deploy the model I would use Flask API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
